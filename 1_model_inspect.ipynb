{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2679b4",
   "metadata": {},
   "source": [
    "# Model Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622d9045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rhd/Documents/Raihan/Work/Data/Model-ML/bot-detection-twitter/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd48af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import humanize\n",
    "\n",
    "\n",
    "def inspect_model_details(model_names):\n",
    "    \"\"\"\n",
    "    Inspect multiple models for a comprehensive comparison, including\n",
    "    architecture, tokenizer, and Hub metadata.\n",
    "    \"\"\"\n",
    "    print(\"üîç COMPREHENSIVE MODEL INSPECTION\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Inisialisasi API untuk mengambil data dari Hugging Face Hub\n",
    "    hf_api = HfApi()\n",
    "    results = {}\n",
    "\n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nüìã Inspecting: {model_name}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        try:\n",
    "            # 1. Inspeksi Konfigurasi\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "            info = {\n",
    "                \"model_type\": config.model_type,\n",
    "                # Detail Arsitektur\n",
    "                \"hidden_size\": getattr(config, \"hidden_size\", \"N/A\"),\n",
    "                \"num_layers\": getattr(config, \"num_hidden_layers\", \"N/A\"),\n",
    "                \"num_heads\": getattr(config, \"num_attention_heads\", \"N/A\"),\n",
    "                \"num_parameters\": (\n",
    "                    humanize.intword(config.num_parameters())\n",
    "                    if hasattr(config, \"num_parameters\")\n",
    "                    and callable(config.num_parameters)\n",
    "                    else \"N/A\"\n",
    "                ),\n",
    "                # Detail Klasifikasi\n",
    "                \"num_labels\": config.num_labels,\n",
    "                \"labels\": dict(config.id2label) if hasattr(config, \"id2label\") else {},\n",
    "                \"problem_type\": getattr(config, \"problem_type\", \"Not specified\"),\n",
    "            }\n",
    "\n",
    "            print(\"   [Architecture]\")\n",
    "            print(f\"   - Model Type: {info['model_type']}\")\n",
    "            print(f\"   - Parameters: {info['num_parameters']}\")\n",
    "            print(\n",
    "                f\"   - Layers: {info['num_layers']}, Hidden Size: {info['hidden_size']}, Heads: {info['num_heads']}\"\n",
    "            )\n",
    "\n",
    "            print(\"\\n   [Classification Task]\")\n",
    "            print(f\"   - Problem Type: {info['problem_type']}\")\n",
    "            print(f\"   - Number of Labels: {info['num_labels']}\")\n",
    "            if info[\"labels\"]:\n",
    "                print(f\"   - Categories: {list(info['labels'].values())}\")\n",
    "\n",
    "            # 2. Inspeksi Tokenizer\n",
    "            try:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                info[\"tokenizer_class\"] = tokenizer.__class__.__name__\n",
    "                info[\"vocab_size\"] = humanize.intword(tokenizer.vocab_size)\n",
    "\n",
    "                print(\"\\n   [Tokenizer]\")\n",
    "                print(f\"   - Class: {info['tokenizer_class']}\")\n",
    "                print(f\"   - Vocabulary Size: {info['vocab_size']}\")\n",
    "            except Exception as tokenizer_error:\n",
    "                print(f\"\\n   [Tokenizer]\")\n",
    "                print(f\"   - ‚ùå Error loading tokenizer: {tokenizer_error}\")\n",
    "                info[\"tokenizer_error\"] = str(tokenizer_error)\n",
    "\n",
    "            # 3. Inspeksi Metadata dari Hugging Face Hub (with better error handling)\n",
    "            try:\n",
    "                model_info_hub = hf_api.model_info(model_name)\n",
    "\n",
    "                # Safe access to attributes\n",
    "                downloads = getattr(model_info_hub, \"downloads\", 0)\n",
    "                likes = getattr(model_info_hub, \"likes\", 0)\n",
    "                last_modified = getattr(model_info_hub, \"lastModified\", None)\n",
    "\n",
    "                info[\"downloads\"] = humanize.intword(downloads) if downloads else \"N/A\"\n",
    "                info[\"likes\"] = humanize.intword(likes) if likes else \"N/A\"\n",
    "\n",
    "                # Safe date formatting\n",
    "                if last_modified:\n",
    "                    if hasattr(last_modified, \"strftime\"):\n",
    "                        info[\"last_modified\"] = last_modified.strftime(\"%Y-%m-%d\")\n",
    "                    else:\n",
    "                        info[\"last_modified\"] = str(last_modified).split(\"T\")[0]\n",
    "                else:\n",
    "                    info[\"last_modified\"] = \"N/A\"\n",
    "\n",
    "                print(\"\\n   [Hub Info]\")\n",
    "                print(f\"   - Downloads: {info['downloads']}\")\n",
    "                print(f\"   - Likes: {info['likes']}\")\n",
    "                print(f\"   - Last Modified: {info['last_modified']}\")\n",
    "\n",
    "            except Exception as hub_error:\n",
    "                print(f\"\\n   [Hub Info]\")\n",
    "                print(f\"   - ‚ùå Error accessing Hub info: {hub_error}\")\n",
    "                info[\"hub_error\"] = str(hub_error)\n",
    "                info[\"downloads\"] = \"N/A\"\n",
    "                info[\"likes\"] = \"N/A\"\n",
    "                info[\"last_modified\"] = \"N/A\"\n",
    "\n",
    "            results[model_name] = info\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing {model_name}: {e}\")\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf1a48",
   "metadata": {},
   "source": [
    "# Sentiment Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9a9b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COMPREHENSIVE MODEL INSPECTION\n",
      "================================================================================\n",
      "\n",
      "üìã Inspecting: Aardiiiiy/indobertweet-base-Indonesian-sentiment-analysis\n",
      "------------------------------------------------------------\n",
      "   [Architecture]\n",
      "   - Model Type: bert\n",
      "   - Parameters: N/A\n",
      "   - Layers: 12, Hidden Size: 768, Heads: 12\n",
      "\n",
      "   [Classification Task]\n",
      "   - Problem Type: single_label_classification\n",
      "   - Number of Labels: 3\n",
      "   - Categories: ['Negative', 'Neutral', 'Positive']\n",
      "   [Architecture]\n",
      "   - Model Type: bert\n",
      "   - Parameters: N/A\n",
      "   - Layers: 12, Hidden Size: 768, Heads: 12\n",
      "\n",
      "   [Classification Task]\n",
      "   - Problem Type: single_label_classification\n",
      "   - Number of Labels: 3\n",
      "   - Categories: ['Negative', 'Neutral', 'Positive']\n",
      "\n",
      "   [Tokenizer]\n",
      "   - Class: BertTokenizerFast\n",
      "   - Vocabulary Size: 31.9 thousand\n",
      "\n",
      "   [Tokenizer]\n",
      "   - Class: BertTokenizerFast\n",
      "   - Vocabulary Size: 31.9 thousand\n",
      "\n",
      "   [Hub Info]\n",
      "   - Downloads: 3.1 thousand\n",
      "   - Likes: 6\n",
      "   - Last Modified: 2025-05-23\n",
      "\n",
      "   [Hub Info]\n",
      "   - Downloads: 3.1 thousand\n",
      "   - Likes: 6\n",
      "   - Last Modified: 2025-05-23\n"
     ]
    }
   ],
   "source": [
    "models_to_compare = [\n",
    "    \"Aardiiiiy/indobertweet-base-Indonesian-sentiment-analysis\",\n",
    "]\n",
    "\n",
    "detailed_results = inspect_model_details(models_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa8c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(\n",
    "        self, model_name=\"Aardiiiiy/indobertweet-base-Indonesian-sentiment-analysis\"\n",
    "    ):\n",
    "        \"\"\"Initialize sentiment analyzer with IndoBERTweet model\"\"\"\n",
    "        print(\"üîÑ Loading IndoBERTweet sentiment model...\")\n",
    "\n",
    "        # Using pipeline (simplest approach)\n",
    "        self.pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "        )\n",
    "\n",
    "        # Load tokenizer and model separately for more control if needed\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "        # Based on our inspection - we know exactly what labels exist\n",
    "        self.sentiment_labels = [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"]\n",
    "\n",
    "        print(\"‚úÖ Sentiment model loaded successfully!\")\n",
    "        print(f\"üîß Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "        print(f\"üè∑Ô∏è Labels: {', '.join(self.sentiment_labels)}\")\n",
    "\n",
    "    def predict_single(self, text):\n",
    "        \"\"\"Predict sentiment for a single text\"\"\"\n",
    "        if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "            return {\"label\": \"NEUTRAL\", \"score\": 0.0, \"confidence\": \"low\"}\n",
    "\n",
    "        try:\n",
    "            result = self.pipe(str(text))\n",
    "            prediction = result[0]\n",
    "\n",
    "            # Add confidence level based on score\n",
    "            if prediction[\"score\"] >= 0.8:\n",
    "                confidence = \"high\"\n",
    "            elif prediction[\"score\"] >= 0.6:\n",
    "                confidence = \"medium\"\n",
    "            else:\n",
    "                confidence = \"low\"\n",
    "\n",
    "            return {\n",
    "                \"label\": prediction[\"label\"],\n",
    "                \"score\": prediction[\"score\"],\n",
    "                \"confidence\": confidence,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting sentiment: {e}\")\n",
    "            return {\"label\": \"NEUTRAL\", \"score\": 0.0, \"confidence\": \"error\"}\n",
    "\n",
    "    def predict_batch(self, texts, batch_size=32):\n",
    "        \"\"\"Predict sentiment for multiple texts efficiently\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Convert to list if pandas Series\n",
    "        if hasattr(texts, \"tolist\"):\n",
    "            texts = texts.tolist()\n",
    "\n",
    "        print(f\"üîÑ Processing {len(texts)} texts for sentiment analysis...\")\n",
    "        print(f\"üìä Batch size: {batch_size}\")\n",
    "\n",
    "        # Process in batches with progress bar\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing sentiment\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "\n",
    "            # Clean batch texts\n",
    "            clean_batch = []\n",
    "            for text in batch:\n",
    "                if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "                    clean_batch.append(\"No Comment\")\n",
    "                else:\n",
    "                    clean_batch.append(str(text))\n",
    "\n",
    "            try:\n",
    "                # Predict batch\n",
    "                batch_results = self.pipe(clean_batch)\n",
    "\n",
    "                # Process results\n",
    "                for j, result in enumerate(batch_results):\n",
    "                    if clean_batch[j] == \"No Comment\":\n",
    "                        results.append(\n",
    "                            {\"label\": \"NEUTRAL\", \"score\": 0.0, \"confidence\": \"low\"}\n",
    "                        )\n",
    "                    else:\n",
    "                        # Add confidence level\n",
    "                        if result[\"score\"] >= 0.8:\n",
    "                            confidence = \"high\"\n",
    "                        elif result[\"score\"] >= 0.6:\n",
    "                            confidence = \"medium\"\n",
    "                        else:\n",
    "                            confidence = \"low\"\n",
    "\n",
    "                        results.append(\n",
    "                            {\n",
    "                                \"label\": result[\"label\"],\n",
    "                                \"score\": result[\"score\"],\n",
    "                                \"confidence\": confidence,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in sentiment batch {i//batch_size + 1}: {e}\")\n",
    "                # Add neutral predictions for failed batch\n",
    "                for _ in range(len(batch)):\n",
    "                    results.append(\n",
    "                        {\"label\": \"NEUTRAL\", \"score\": 0.0, \"confidence\": \"error\"}\n",
    "                    )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_results(self, results):\n",
    "        \"\"\"Analyze and display sentiment analysis results\"\"\"\n",
    "        total = len(results)\n",
    "\n",
    "        print(f\"\\nüìä SENTIMENT ANALYSIS SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìù Total texts: {total}\")\n",
    "\n",
    "        # Sentiment distribution\n",
    "        sentiment_counts = {\"POSITIVE\": 0, \"NEGATIVE\": 0, \"NEUTRAL\": 0}\n",
    "        for result in results:\n",
    "            label = result[\"label\"]\n",
    "            sentiment_counts[label] = sentiment_counts.get(label, 0) + 1\n",
    "\n",
    "        print(f\"\\nüí≠ Sentiment Distribution:\")\n",
    "        for sentiment, count in sentiment_counts.items():\n",
    "            percentage = (count / total) * 100\n",
    "            emoji = (\n",
    "                \"üòä\"\n",
    "                if sentiment == \"POSITIVE\"\n",
    "                else \"üòû\" if sentiment == \"NEGATIVE\" else \"üòê\"\n",
    "            )\n",
    "            print(f\"   {emoji} {sentiment}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Confidence distribution\n",
    "        conf_counts = {\"high\": 0, \"medium\": 0, \"low\": 0, \"error\": 0}\n",
    "        for result in results:\n",
    "            conf_counts[result[\"confidence\"]] += 1\n",
    "\n",
    "        print(f\"\\nüéØ Confidence Distribution:\")\n",
    "        for conf, count in conf_counts.items():\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"   {conf.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"sentiment_distribution\": sentiment_counts,\n",
    "            \"confidence_distribution\": conf_counts,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ac7100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing SentimentAnalyzer...\n",
      "üîÑ Loading IndoBERTweet sentiment model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sentiment model loaded successfully!\n",
      "üîß Using device: CPU\n",
      "üè∑Ô∏è Labels: NEGATIVE, NEUTRAL, POSITIVE\n",
      "\n",
      "üß™ Testing individual sentiment predictions:\n",
      "------------------------------------------------------------\n",
      "Text: 'Gimana sih @layananinternet, sinyalnya ilang-ilangan mulu dari pagi di daerah Bekasi. Mana lagi butuh buat kerjaan #internetdown'\n",
      "   üí≠ Sentiment: üòê Negative (0.970) - high\n",
      "\n",
      "Text: 'Sumpah, vibe-nya cozy abis buat nugas. Kopinya juga aje gile mantep. Fix bakal jadi langganan! ‚ú®'\n",
      "   üí≠ Sentiment: üòê Positive (0.997) - high\n",
      "\n",
      "Text: 'Overall experience-nya lumayan sih, cuman servicenya agak lama. Nunggunya ampe stengah jam sndiri.'\n",
      "   üí≠ Sentiment: üòê Neutral (0.995) - high\n",
      "\n",
      "Text: 'Desain HP-nya keren, kameranya juga oke. TAPI KENAPA BATERAINYA BOROS BANGET?! Baru setengah hari udah abis üò≠'\n",
      "   üí≠ Sentiment: üòê Neutral (0.841) - high\n",
      "\n",
      "Text: 'Ada yg tau info konser terbaru bulan ini gaes?'\n",
      "   üí≠ Sentiment: üòê Neutral (0.998) - high\n",
      "\n",
      "‚úÖ SentimentAnalyzer test completed!\n",
      "Text: 'Gimana sih @layananinternet, sinyalnya ilang-ilangan mulu dari pagi di daerah Bekasi. Mana lagi butuh buat kerjaan #internetdown'\n",
      "   üí≠ Sentiment: üòê Negative (0.970) - high\n",
      "\n",
      "Text: 'Sumpah, vibe-nya cozy abis buat nugas. Kopinya juga aje gile mantep. Fix bakal jadi langganan! ‚ú®'\n",
      "   üí≠ Sentiment: üòê Positive (0.997) - high\n",
      "\n",
      "Text: 'Overall experience-nya lumayan sih, cuman servicenya agak lama. Nunggunya ampe stengah jam sndiri.'\n",
      "   üí≠ Sentiment: üòê Neutral (0.995) - high\n",
      "\n",
      "Text: 'Desain HP-nya keren, kameranya juga oke. TAPI KENAPA BATERAINYA BOROS BANGET?! Baru setengah hari udah abis üò≠'\n",
      "   üí≠ Sentiment: üòê Neutral (0.841) - high\n",
      "\n",
      "Text: 'Ada yg tau info konser terbaru bulan ini gaes?'\n",
      "   üí≠ Sentiment: üòê Neutral (0.998) - high\n",
      "\n",
      "‚úÖ SentimentAnalyzer test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test SentimentAnalyzer only\n",
    "\n",
    "print(\"üöÄ Testing SentimentAnalyzer...\")\n",
    "\n",
    "# Initialize\n",
    "try:\n",
    "    sentiment_analyzer = SentimentAnalyzer()\n",
    "\n",
    "    # Test with some examples\n",
    "    test_texts = [\n",
    "        \"Gimana sih @layananinternet, sinyalnya ilang-ilangan mulu dari pagi di daerah Bekasi. Mana lagi butuh buat kerjaan #internetdown\",\n",
    "        \"Sumpah, vibe-nya cozy abis buat nugas. Kopinya juga aje gile mantep. Fix bakal jadi langganan! ‚ú®\",\n",
    "        \"Overall experience-nya lumayan sih, cuman servicenya agak lama. Nunggunya ampe stengah jam sndiri.\",\n",
    "        \"Desain HP-nya keren, kameranya juga oke. TAPI KENAPA BATERAINYA BOROS BANGET?! Baru setengah hari udah abis üò≠\",\n",
    "        \"Ada yg tau info konser terbaru bulan ini gaes?\",\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüß™ Testing individual sentiment predictions:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for text in test_texts:\n",
    "        result = sentiment_analyzer.predict_single(text)\n",
    "        sentiment_emoji = (\n",
    "            \"üòä\"\n",
    "            if result[\"label\"] == \"POSITIVE\"\n",
    "            else \"üòû\" if result[\"label\"] == \"NEGATIVE\" else \"üòê\"\n",
    "        )\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(\n",
    "            f\"   üí≠ Sentiment: {sentiment_emoji} {result['label']} ({result['score']:.3f}) - {result['confidence']}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "    print(\"‚úÖ SentimentAnalyzer test completed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in SentimentAnalyzer: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad4fcf",
   "metadata": {},
   "source": [
    "# Emotion Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "923a9dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COMPREHENSIVE MODEL INSPECTION\n",
      "================================================================================\n",
      "\n",
      "üìã Inspecting: Aardiiiiy/EmoSense-ID-Indonesian-Emotion-Classifier\n",
      "------------------------------------------------------------\n",
      "   [Architecture]\n",
      "   - Model Type: bert\n",
      "   - Parameters: N/A\n",
      "   - Layers: 12, Hidden Size: 768, Heads: 12\n",
      "\n",
      "   [Classification Task]\n",
      "   - Problem Type: single_label_classification\n",
      "   - Number of Labels: 8\n",
      "   - Categories: ['Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust']\n",
      "   [Architecture]\n",
      "   - Model Type: bert\n",
      "   - Parameters: N/A\n",
      "   - Layers: 12, Hidden Size: 768, Heads: 12\n",
      "\n",
      "   [Classification Task]\n",
      "   - Problem Type: single_label_classification\n",
      "   - Number of Labels: 8\n",
      "   - Categories: ['Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust']\n",
      "\n",
      "   [Tokenizer]\n",
      "   - Class: BertTokenizerFast\n",
      "   - Vocabulary Size: 30.5 thousand\n",
      "\n",
      "   [Tokenizer]\n",
      "   - Class: BertTokenizerFast\n",
      "   - Vocabulary Size: 30.5 thousand\n",
      "\n",
      "   [Hub Info]\n",
      "   - Downloads: 2.0 thousand\n",
      "   - Likes: 2\n",
      "   - Last Modified: 2025-05-09\n",
      "\n",
      "   [Hub Info]\n",
      "   - Downloads: 2.0 thousand\n",
      "   - Likes: 2\n",
      "   - Last Modified: 2025-05-09\n"
     ]
    }
   ],
   "source": [
    "models_to_compare = [\n",
    "    \"Aardiiiiy/EmoSense-ID-Indonesian-Emotion-Classifier\",\n",
    "]\n",
    "\n",
    "detailed_results = inspect_model_details(models_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328c2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionAnalyzer:\n",
    "    def __init__(\n",
    "        self, model_name=\"Aardiiiiy/EmoSense-ID-Indonesian-Emotion-Classifier\"\n",
    "    ):\n",
    "        \"\"\"Initialize emotion analyzer with EmoSense model\"\"\"\n",
    "        print(\"üîÑ Loading EmoSense Indonesian emotion model...\")\n",
    "\n",
    "        # Using pipeline (simplest approach)\n",
    "        self.pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "        )\n",
    "\n",
    "        # Load tokenizer and model separately for more control if needed\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "        # Based on our inspection - we know exactly what labels exist (Plutchik's 8 emotions)\n",
    "        self.emotion_labels = [\n",
    "            \"Anger\",\n",
    "            \"Anticipation\",\n",
    "            \"Disgust\",\n",
    "            \"Fear\",\n",
    "            \"Joy\",\n",
    "            \"Sadness\",\n",
    "            \"Surprise\",\n",
    "            \"Trust\",\n",
    "        ]\n",
    "\n",
    "        # Emotion emojis for better display\n",
    "        self.emotion_emojis = {\n",
    "            \"Anger\": \"üò°\",\n",
    "            \"Anticipation\": \"ü§î\",\n",
    "            \"Disgust\": \"ü§¢\",\n",
    "            \"Fear\": \"üò®\",\n",
    "            \"Joy\": \"üòä\",\n",
    "            \"Sadness\": \"üò¢\",\n",
    "            \"Surprise\": \"üò≤\",\n",
    "            \"Trust\": \"ü§ù\",\n",
    "        }\n",
    "\n",
    "        print(\"‚úÖ Emotion model loaded successfully!\")\n",
    "        print(f\"üîß Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "        print(f\"üé≠ Emotions: {', '.join(self.emotion_labels)}\")\n",
    "\n",
    "    def predict_single(self, text):\n",
    "        \"\"\"Predict emotion for a single text\"\"\"\n",
    "        if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "            return {\"label\": \"Trust\", \"score\": 0.0, \"confidence\": \"low\"}\n",
    "\n",
    "        try:\n",
    "            result = self.pipe(str(text))\n",
    "            prediction = result[0]\n",
    "\n",
    "            # Add confidence level based on score\n",
    "            if prediction[\"score\"] >= 0.8:\n",
    "                confidence = \"high\"\n",
    "            elif prediction[\"score\"] >= 0.6:\n",
    "                confidence = \"medium\"\n",
    "            else:\n",
    "                confidence = \"low\"\n",
    "\n",
    "            return {\n",
    "                \"label\": prediction[\"label\"],\n",
    "                \"score\": prediction[\"score\"],\n",
    "                \"confidence\": confidence,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting emotion: {e}\")\n",
    "            return {\"label\": \"Trust\", \"score\": 0.0, \"confidence\": \"error\"}\n",
    "\n",
    "    def predict_batch(self, texts, batch_size=32):\n",
    "        \"\"\"Predict emotion for multiple texts efficiently\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Convert to list if pandas Series\n",
    "        if hasattr(texts, \"tolist\"):\n",
    "            texts = texts.tolist()\n",
    "\n",
    "        print(f\"üîÑ Processing {len(texts)} texts for emotion analysis...\")\n",
    "        print(f\"üìä Batch size: {batch_size}\")\n",
    "\n",
    "        # Process in batches with progress bar\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing emotions\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "\n",
    "            # Clean batch texts\n",
    "            clean_batch = []\n",
    "            for text in batch:\n",
    "                if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "                    clean_batch.append(\"No Comment\")\n",
    "                else:\n",
    "                    clean_batch.append(str(text))\n",
    "\n",
    "            try:\n",
    "                # Predict batch\n",
    "                batch_results = self.pipe(clean_batch)\n",
    "\n",
    "                # Process results\n",
    "                for j, result in enumerate(batch_results):\n",
    "                    if clean_batch[j] == \"No Comment\":\n",
    "                        results.append(\n",
    "                            {\"label\": \"Trust\", \"score\": 0.0, \"confidence\": \"low\"}\n",
    "                        )\n",
    "                    else:\n",
    "                        # Add confidence level\n",
    "                        if result[\"score\"] >= 0.8:\n",
    "                            confidence = \"high\"\n",
    "                        elif result[\"score\"] >= 0.6:\n",
    "                            confidence = \"medium\"\n",
    "                        else:\n",
    "                            confidence = \"low\"\n",
    "\n",
    "                        results.append(\n",
    "                            {\n",
    "                                \"label\": result[\"label\"],\n",
    "                                \"score\": result[\"score\"],\n",
    "                                \"confidence\": confidence,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in emotion batch {i//batch_size + 1}: {e}\")\n",
    "                # Add default predictions for failed batch\n",
    "                for _ in range(len(batch)):\n",
    "                    results.append(\n",
    "                        {\"label\": \"Trust\", \"score\": 0.0, \"confidence\": \"error\"}\n",
    "                    )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_results(self, results):\n",
    "        \"\"\"Analyze and display emotion analysis results\"\"\"\n",
    "        total = len(results)\n",
    "\n",
    "        print(f\"\\nüìä EMOTION ANALYSIS SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìù Total texts: {total}\")\n",
    "\n",
    "        # Emotion distribution\n",
    "        emotion_counts = {}\n",
    "        for emotion in self.emotion_labels:\n",
    "            emotion_counts[emotion] = 0\n",
    "\n",
    "        for result in results:\n",
    "            label = result[\"label\"]\n",
    "            emotion_counts[label] = emotion_counts.get(label, 0) + 1\n",
    "\n",
    "        print(f\"\\nüé≠ Emotion Distribution:\")\n",
    "        # Sort by count, descending\n",
    "        sorted_emotions = sorted(\n",
    "            emotion_counts.items(), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        for emotion, count in sorted_emotions:\n",
    "            percentage = (count / total) * 100\n",
    "            emoji = self.emotion_emojis.get(emotion, \"üé≠\")\n",
    "            print(f\"   {emoji} {emotion}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Confidence distribution\n",
    "        conf_counts = {\"high\": 0, \"medium\": 0, \"low\": 0, \"error\": 0}\n",
    "        for result in results:\n",
    "            conf_counts[result[\"confidence\"]] += 1\n",
    "\n",
    "        print(f\"\\nüéØ Confidence Distribution:\")\n",
    "        for conf, count in conf_counts.items():\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"   {conf.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"emotion_distribution\": emotion_counts,\n",
    "            \"confidence_distribution\": conf_counts,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ee5480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing EmotionAnalyzer...\n",
      "üîÑ Loading EmoSense Indonesian emotion model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Emotion model loaded successfully!\n",
      "üîß Using device: CPU\n",
      "üé≠ Emotions: Anger, Anticipation, Disgust, Fear, Joy, Sadness, Surprise, Trust\n",
      "\n",
      "üß™ Testing individual emotion predictions:\n",
      "------------------------------------------------------------\n",
      "Text: 'Saya sangat marah dengan pelayanan ini!'\n",
      "   Expected: Anger\n",
      "   üé≠ Emotion: üò° Anger (0.981) - high\n",
      "\n",
      "Text: 'Wah senang sekali dapat hadiah ini!'\n",
      "   Expected: Joy\n",
      "   üé≠ Emotion: üòä Joy (0.992) - high\n",
      "\n",
      "Text: 'Saya merasa sedih sekali hari ini'\n",
      "   Expected: Sadness\n",
      "   üé≠ Emotion: üò¢ Sadness (0.993) - high\n",
      "\n",
      "Text: 'Ngeri banget nonton film horror tadi'\n",
      "   Expected: Fear\n",
      "   üé≠ Emotion: üò® Fear (0.979) - high\n",
      "\n",
      "Text: 'Kaget banget ternyata dia datang!'\n",
      "   Expected: Surprise\n",
      "   üé≠ Emotion: üò≤ Surprise (0.994) - high\n",
      "\n",
      "Text: 'Saya sangat marah dengan pelayanan ini!'\n",
      "   Expected: Anger\n",
      "   üé≠ Emotion: üò° Anger (0.981) - high\n",
      "\n",
      "Text: 'Wah senang sekali dapat hadiah ini!'\n",
      "   Expected: Joy\n",
      "   üé≠ Emotion: üòä Joy (0.992) - high\n",
      "\n",
      "Text: 'Saya merasa sedih sekali hari ini'\n",
      "   Expected: Sadness\n",
      "   üé≠ Emotion: üò¢ Sadness (0.993) - high\n",
      "\n",
      "Text: 'Ngeri banget nonton film horror tadi'\n",
      "   Expected: Fear\n",
      "   üé≠ Emotion: üò® Fear (0.979) - high\n",
      "\n",
      "Text: 'Kaget banget ternyata dia datang!'\n",
      "   Expected: Surprise\n",
      "   üé≠ Emotion: üò≤ Surprise (0.994) - high\n",
      "\n",
      "Text: 'Jijik banget lihat yang begitu'\n",
      "   Expected: Disgust\n",
      "   üé≠ Emotion: ü§¢ Disgust (0.990) - high\n",
      "\n",
      "Text: 'Saya percaya sepenuhnya dengan tim ini'\n",
      "   Expected: Trust\n",
      "   üé≠ Emotion: ü§ù Trust (0.993) - high\n",
      "\n",
      "Text: 'Tidak sabar menunggu acara besok!'\n",
      "   Expected: Anticipation\n",
      "   üé≠ Emotion: ü§î Anticipation (0.989) - high\n",
      "\n",
      "‚úÖ EmotionAnalyzer test completed!\n",
      "Text: 'Jijik banget lihat yang begitu'\n",
      "   Expected: Disgust\n",
      "   üé≠ Emotion: ü§¢ Disgust (0.990) - high\n",
      "\n",
      "Text: 'Saya percaya sepenuhnya dengan tim ini'\n",
      "   Expected: Trust\n",
      "   üé≠ Emotion: ü§ù Trust (0.993) - high\n",
      "\n",
      "Text: 'Tidak sabar menunggu acara besok!'\n",
      "   Expected: Anticipation\n",
      "   üé≠ Emotion: ü§î Anticipation (0.989) - high\n",
      "\n",
      "‚úÖ EmotionAnalyzer test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test EmotionAnalyzer only\n",
    "\n",
    "print(\"üöÄ Testing EmotionAnalyzer...\")\n",
    "\n",
    "# Initialize\n",
    "try:\n",
    "    emotion_analyzer = EmotionAnalyzer()\n",
    "\n",
    "    # Test with emotion-specific examples\n",
    "    emotion_test_texts = [\n",
    "        (\"Saya sangat marah dengan pelayanan ini!\", \"Expected: Anger\"),\n",
    "        (\"Wah senang sekali dapat hadiah ini!\", \"Expected: Joy\"),\n",
    "        (\"Saya merasa sedih sekali hari ini\", \"Expected: Sadness\"),\n",
    "        (\"Ngeri banget nonton film horror tadi\", \"Expected: Fear\"),\n",
    "        (\"Kaget banget ternyata dia datang!\", \"Expected: Surprise\"),\n",
    "        (\"Jijik banget lihat yang begitu\", \"Expected: Disgust\"),\n",
    "        (\"Saya percaya sepenuhnya dengan tim ini\", \"Expected: Trust\"),\n",
    "        (\"Tidak sabar menunggu acara besok!\", \"Expected: Anticipation\"),\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüß™ Testing individual emotion predictions:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for text, expected in emotion_test_texts:\n",
    "        result = emotion_analyzer.predict_single(text)\n",
    "        emotion_emoji = emotion_analyzer.emotion_emojis.get(result[\"label\"], \"üé≠\")\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"   {expected}\")\n",
    "        print(\n",
    "            f\"   üé≠ Emotion: {emotion_emoji} {result['label']} ({result['score']:.3f}) - {result['confidence']}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "    print(\"‚úÖ EmotionAnalyzer test completed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in EmotionAnalyzer: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a0f9e",
   "metadata": {},
   "source": [
    "# Hate speech Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd50fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COMPREHENSIVE MODEL INSPECTION\n",
      "================================================================================\n",
      "\n",
      "üìã Inspecting: PaceKW/bert-multilabel-indonesian-hate-speech\n",
      "------------------------------------------------------------\n",
      "   [Architecture]\n",
      "   - Model Type: bert\n",
      "   - Parameters: N/A\n",
      "   - Layers: 12, Hidden Size: 768, Heads: 12\n",
      "\n",
      "   [Classification Task]\n",
      "   - Problem Type: multi_label_classification\n",
      "   - Number of Labels: 12\n",
      "   - Categories: ['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong']\n",
      "   [Architecture]\n",
      "   - Model Type: bert\n",
      "   - Parameters: N/A\n",
      "   - Layers: 12, Hidden Size: 768, Heads: 12\n",
      "\n",
      "   [Classification Task]\n",
      "   - Problem Type: multi_label_classification\n",
      "   - Number of Labels: 12\n",
      "   - Categories: ['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong']\n",
      "\n",
      "   [Tokenizer]\n",
      "   - Class: BertTokenizerFast\n",
      "   - Vocabulary Size: 31.9 thousand\n",
      "\n",
      "   [Tokenizer]\n",
      "   - Class: BertTokenizerFast\n",
      "   - Vocabulary Size: 31.9 thousand\n",
      "\n",
      "   [Hub Info]\n",
      "   - Downloads: 47\n",
      "   - Likes: N/A\n",
      "   - Last Modified: 2025-05-16\n",
      "\n",
      "üìã Inspecting: PaceKW/indobert-base-p1-multilabel-indonesian-hate-speech-new\n",
      "------------------------------------------------------------\n",
      "\n",
      "   [Hub Info]\n",
      "   - Downloads: 47\n",
      "   - Likes: N/A\n",
      "   - Last Modified: 2025-05-16\n",
      "\n",
      "üìã Inspecting: PaceKW/indobert-base-p1-multilabel-indonesian-hate-speech-new\n",
      "------------------------------------------------------------\n",
      "   [Architecture]\n",
      "   - Model Type: bert\n",
      "   - Parameters: N/A\n",
      "   - Layers: 12, Hidden Size: 768, Heads: 12\n",
      "\n",
      "   [Classification Task]\n",
      "   - Problem Type: multi_label_classification\n",
      "   - Number of Labels: 12\n",
      "   - Categories: ['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong']\n",
      "   [Architecture]\n",
      "   - Model Type: bert\n",
      "   - Parameters: N/A\n",
      "   - Layers: 12, Hidden Size: 768, Heads: 12\n",
      "\n",
      "   [Classification Task]\n",
      "   - Problem Type: multi_label_classification\n",
      "   - Number of Labels: 12\n",
      "   - Categories: ['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong']\n",
      "\n",
      "   [Tokenizer]\n",
      "   - Class: BertTokenizerFast\n",
      "   - Vocabulary Size: 30.5 thousand\n",
      "\n",
      "   [Tokenizer]\n",
      "   - Class: BertTokenizerFast\n",
      "   - Vocabulary Size: 30.5 thousand\n",
      "\n",
      "   [Hub Info]\n",
      "   - Downloads: 200\n",
      "   - Likes: N/A\n",
      "   - Last Modified: 2025-05-22\n",
      "\n",
      "   [Hub Info]\n",
      "   - Downloads: 200\n",
      "   - Likes: N/A\n",
      "   - Last Modified: 2025-05-22\n"
     ]
    }
   ],
   "source": [
    "models_to_compare = [\n",
    "    \"PaceKW/bert-multilabel-indonesian-hate-speech\",\n",
    "    \"PaceKW/indobert-base-p1-multilabel-indonesian-hate-speech-new\"\n",
    "]\n",
    "\n",
    "detailed_results = inspect_model_details(models_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8ce28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechAnalyzer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"PaceKW/distilbert-base-multilingual-cased-multilabel-indonesian-hate-speech\",\n",
    "    ):\n",
    "        \"\"\"Initialize hate speech analyzer\"\"\"\n",
    "        print(\"üîÑ Loading Indonesian Hate Speech model...\")\n",
    "\n",
    "        # Using pipeline (simplest approach)\n",
    "        self.pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            return_all_scores=True,  # Important for multilabel\n",
    "        )\n",
    "\n",
    "        # Load tokenizer and model separately for more control if needed\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "        # Based on our inspection - we know exactly what labels exist\n",
    "        self.hate_categories = [\n",
    "            \"HS\",\n",
    "            \"Abusive\",\n",
    "            \"HS_Individual\",\n",
    "            \"HS_Group\",\n",
    "            \"HS_Religion\",\n",
    "            \"HS_Race\",\n",
    "            \"HS_Physical\",\n",
    "            \"HS_Gender\",\n",
    "            \"HS_Other\",\n",
    "            \"HS_Weak\",\n",
    "            \"HS_Moderate\",\n",
    "            \"HS_Strong\",\n",
    "        ]\n",
    "\n",
    "        print(\"‚úÖ Hate speech model loaded successfully!\")\n",
    "        print(f\"üîß Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "        print(f\"üè∑Ô∏è Categories: {', '.join(self.hate_categories)}\")\n",
    "\n",
    "    def predict_single(self, text, threshold=0.5):\n",
    "        \"\"\"Predict hate speech for a single text\"\"\"\n",
    "        if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "            return {\n",
    "                \"is_hate_speech\": False,\n",
    "                \"categories\": [],\n",
    "                \"scores\": {},\n",
    "                \"max_score\": 0.0,\n",
    "                \"confidence\": \"low\",\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Get predictions for all labels\n",
    "            results = self.pipe(str(text))\n",
    "\n",
    "            # Debug: Print hasil untuk lihat struktur\n",
    "            print(f\"Debug - Raw result type: {type(results)}\")\n",
    "            print(f\"Debug - Raw result: {results}\")\n",
    "\n",
    "            # Handle different output formats\n",
    "            if isinstance(results, list):\n",
    "                # Jika hasil adalah list of lists (nested)\n",
    "                if len(results) > 0 and isinstance(results[0], list):\n",
    "                    predictions = results[0]  # Ambil list pertama\n",
    "                else:\n",
    "                    predictions = results  # Sudah format yang benar\n",
    "            else:\n",
    "                predictions = [results]  # Bungkus dalam list jika bukan list\n",
    "\n",
    "            # Process multilabel results\n",
    "            active_categories = []\n",
    "            all_scores = {}\n",
    "            max_score = 0.0\n",
    "\n",
    "            for prediction in predictions:\n",
    "                # Handle different key formats\n",
    "                if isinstance(prediction, dict):\n",
    "                    if \"label\" in prediction and \"score\" in prediction:\n",
    "                        label = prediction[\"label\"]\n",
    "                        score = prediction[\"score\"]\n",
    "                    elif \"LABEL\" in prediction and \"SCORE\" in prediction:\n",
    "                        label = prediction[\"LABEL\"]\n",
    "                        score = prediction[\"SCORE\"]\n",
    "                    else:\n",
    "                        print(f\"Debug - Unknown prediction format: {prediction}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"Debug - Unexpected prediction type: {type(prediction)}\")\n",
    "                    continue\n",
    "\n",
    "                all_scores[label] = score\n",
    "                max_score = max(max_score, score)\n",
    "\n",
    "                # Add to active categories if above threshold\n",
    "                if score >= threshold:\n",
    "                    active_categories.append(label)\n",
    "\n",
    "            # Determine if hate speech detected\n",
    "            is_hate_speech = len(active_categories) > 0\n",
    "\n",
    "            # Confidence based on max score\n",
    "            if max_score >= 0.8:\n",
    "                confidence = \"high\"\n",
    "            elif max_score >= 0.6:\n",
    "                confidence = \"medium\"\n",
    "            else:\n",
    "                confidence = \"low\"\n",
    "\n",
    "            return {\n",
    "                \"is_hate_speech\": is_hate_speech,\n",
    "                \"categories\": active_categories,\n",
    "                \"scores\": all_scores,\n",
    "                \"max_score\": max_score,\n",
    "                \"confidence\": confidence,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting hate speech: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "            return {\n",
    "                \"is_hate_speech\": False,\n",
    "                \"categories\": [],\n",
    "                \"scores\": {},\n",
    "                \"max_score\": 0.0,\n",
    "                \"confidence\": \"error\",\n",
    "            }\n",
    "\n",
    "    # ... rest of the methods remain the same ...\n",
    "    def predict_batch(self, texts, batch_size=16, threshold=0.5):\n",
    "        \"\"\"Predict hate speech for multiple texts efficiently\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Convert to list if pandas Series\n",
    "        if hasattr(texts, \"tolist\"):\n",
    "            texts = texts.tolist()\n",
    "\n",
    "        print(f\"üîÑ Processing {len(texts)} texts for hate speech analysis...\")\n",
    "        print(f\"üìä Threshold: {threshold} | Batch size: {batch_size}\")\n",
    "\n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing hate speech\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "\n",
    "            # Process each text in batch\n",
    "            for text in batch:\n",
    "                result = self.predict_single(text, threshold)\n",
    "                results.append(result)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ffde6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing HateSpeechAnalyzer...\n",
      "üîÑ Loading Indonesian Hate Speech model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/Users/rhd/Documents/Raihan/Work/Data/Model-ML/bot-detection-twitter/.venv/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "/Users/rhd/Documents/Raihan/Work/Data/Model-ML/bot-detection-twitter/.venv/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hate speech model loaded successfully!\n",
      "üîß Using device: CPU\n",
      "üè∑Ô∏è Categories: HS, Abusive, HS_Individual, HS_Group, HS_Religion, HS_Race, HS_Physical, HS_Gender, HS_Other, HS_Weak, HS_Moderate, HS_Strong\n",
      "\n",
      "üß™ Testing individual hate speech predictions:\n",
      "------------------------------------------------------------\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.015548990108072758}, {'label': 'Abusive', 'score': 0.0038112031761556864}, {'label': 'HS_Individual', 'score': 0.012012024410068989}, {'label': 'HS_Group', 'score': 0.004762946628034115}, {'label': 'HS_Religion', 'score': 0.0032223816961050034}, {'label': 'HS_Race', 'score': 0.004168652463704348}, {'label': 'HS_Physical', 'score': 0.00048205480561591685}, {'label': 'HS_Gender', 'score': 0.000542783469427377}, {'label': 'HS_Other', 'score': 0.008541245013475418}, {'label': 'HS_Weak', 'score': 0.00872738379985094}, {'label': 'HS_Moderate', 'score': 0.0030228530522435904}, {'label': 'HS_Strong', 'score': 0.0023450893349945545}]]\n",
      "Text: 'Selamat pagi semua!'\n",
      "   Expected: Clean\n",
      "   üö® Hate Speech: ‚úÖ NO (0.016) - low\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.008176149800419807}, {'label': 'Abusive', 'score': 0.0027387477457523346}, {'label': 'HS_Individual', 'score': 0.006026771385222673}, {'label': 'HS_Group', 'score': 0.002971058012917638}, {'label': 'HS_Religion', 'score': 0.0020135389640927315}, {'label': 'HS_Race', 'score': 0.002004530979320407}, {'label': 'HS_Physical', 'score': 0.0003330110339447856}, {'label': 'HS_Gender', 'score': 0.0003693913167808205}, {'label': 'HS_Other', 'score': 0.004670690279453993}, {'label': 'HS_Weak', 'score': 0.004574993159621954}, {'label': 'HS_Moderate', 'score': 0.002176556270569563}, {'label': 'HS_Strong', 'score': 0.0011345385573804379}]]\n",
      "Text: 'Terima kasih atas bantuannya'\n",
      "   Expected: Clean\n",
      "   üö® Hate Speech: ‚úÖ NO (0.008) - low\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.026395505294203758}, {'label': 'Abusive', 'score': 0.01087238546460867}, {'label': 'HS_Individual', 'score': 0.02919437736272812}, {'label': 'HS_Group', 'score': 0.001898844144307077}, {'label': 'HS_Religion', 'score': 0.0012790595646947622}, {'label': 'HS_Race', 'score': 0.0007396867731586099}, {'label': 'HS_Physical', 'score': 0.0002988287014886737}, {'label': 'HS_Gender', 'score': 0.0003016763657797128}, {'label': 'HS_Other', 'score': 0.026694944128394127}, {'label': 'HS_Weak', 'score': 0.02115461602807045}, {'label': 'HS_Moderate', 'score': 0.0018761014798656106}, {'label': 'HS_Strong', 'score': 0.0008029469754546881}]]\n",
      "Text: 'Dasar bodoh tidak tahu apa-apa'\n",
      "   Expected: Abusive/HS_Individual\n",
      "   üö® Hate Speech: ‚úÖ NO (0.029) - low\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.035234544426202774}, {'label': 'Abusive', 'score': 0.0023976864758878946}, {'label': 'HS_Individual', 'score': 0.003471940755844116}, {'label': 'HS_Group', 'score': 0.027873802930116653}, {'label': 'HS_Religion', 'score': 0.03345653414726257}, {'label': 'HS_Race', 'score': 0.005262589547783136}, {'label': 'HS_Physical', 'score': 0.00030912039801478386}, {'label': 'HS_Gender', 'score': 0.000431761727668345}, {'label': 'HS_Other', 'score': 0.0027212698478251696}, {'label': 'HS_Weak', 'score': 0.0024995950516313314}, {'label': 'HS_Moderate', 'score': 0.019558433443307877}, {'label': 'HS_Strong', 'score': 0.003014323301613331}]]\n",
      "Text: 'Agama kalian sesat semua'\n",
      "   Expected: HS_Religion/HS_Group\n",
      "   üö® Hate Speech: ‚úÖ NO (0.035) - low\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.015162281692028046}, {'label': 'Abusive', 'score': 0.0033614214044064283}, {'label': 'HS_Individual', 'score': 0.007615985814481974}, {'label': 'HS_Group', 'score': 0.006300174631178379}, {'label': 'HS_Religion', 'score': 0.00281955162063241}, {'label': 'HS_Race', 'score': 0.004170163068920374}, {'label': 'HS_Physical', 'score': 0.0004234279622323811}, {'label': 'HS_Gender', 'score': 0.0005079182446934283}, {'label': 'HS_Other', 'score': 0.01000986061990261}, {'label': 'HS_Weak', 'score': 0.0071112168952822685}, {'label': 'HS_Moderate', 'score': 0.004573638550937176}, {'label': 'HS_Strong', 'score': 0.0018083543982356787}]]\n",
      "Text: 'Perempuan memang inferior'\n",
      "   Expected: HS_Gender\n",
      "   üö® Hate Speech: ‚úÖ NO (0.015) - low\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.8648852705955505}, {'label': 'Abusive', 'score': 0.9882041215896606}, {'label': 'HS_Individual', 'score': 0.7469054460525513}, {'label': 'HS_Group', 'score': 0.033975034952163696}, {'label': 'HS_Religion', 'score': 0.02406526356935501}, {'label': 'HS_Race', 'score': 0.006647973787039518}, {'label': 'HS_Physical', 'score': 0.10878635197877884}, {'label': 'HS_Gender', 'score': 0.06266186386346817}, {'label': 'HS_Other', 'score': 0.16982345283031464}, {'label': 'HS_Weak', 'score': 0.7884725332260132}, {'label': 'HS_Moderate', 'score': 0.04536602273583412}, {'label': 'HS_Strong', 'score': 0.0024157268926501274}]]\n",
      "Text: 'Orang ras itu memang jelek'\n",
      "   Expected: HS_Race\n",
      "   üö® Hate Speech: üö® YES (0.988) - high\n",
      "   üè∑Ô∏è Categories: HS, Abusive, HS_Individual, HS_Weak\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.010290510021150112}, {'label': 'Abusive', 'score': 0.786493718624115}, {'label': 'HS_Individual', 'score': 0.013693682849407196}, {'label': 'HS_Group', 'score': 0.001881378935649991}, {'label': 'HS_Religion', 'score': 0.0006591433193534613}, {'label': 'HS_Race', 'score': 0.0010207424638792872}, {'label': 'HS_Physical', 'score': 0.0029774168506264687}, {'label': 'HS_Gender', 'score': 0.002514052204787731}, {'label': 'HS_Other', 'score': 0.007287383545190096}, {'label': 'HS_Weak', 'score': 0.014722837135195732}, {'label': 'HS_Moderate', 'score': 0.002063001738861203}, {'label': 'HS_Strong', 'score': 0.00042156342533417046}]]\n",
      "Text: 'Bunuh saja dia'\n",
      "   Expected: HS_Strong\n",
      "   üö® Hate Speech: üö® YES (0.786) - medium\n",
      "   üè∑Ô∏è Categories: Abusive\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.01610260270535946}, {'label': 'Abusive', 'score': 0.08261874318122864}, {'label': 'HS_Individual', 'score': 0.017462853342294693}, {'label': 'HS_Group', 'score': 0.0016208930173888803}, {'label': 'HS_Religion', 'score': 0.0009445871692150831}, {'label': 'HS_Race', 'score': 0.0006024299073033035}, {'label': 'HS_Physical', 'score': 0.0005559123819693923}, {'label': 'HS_Gender', 'score': 0.0005030823522247374}, {'label': 'HS_Other', 'score': 0.009527668356895447}, {'label': 'HS_Weak', 'score': 0.016417071223258972}, {'label': 'HS_Moderate', 'score': 0.0012569507816806436}, {'label': 'HS_Strong', 'score': 0.0003428662894293666}]]\n",
      "Text: 'Agak aneh sih orangnya'\n",
      "   Expected: HS_Weak\n",
      "   üö® Hate Speech: ‚úÖ NO (0.083) - low\n",
      "\n",
      "‚úÖ HateSpeechAnalyzer test completed!\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.015548990108072758}, {'label': 'Abusive', 'score': 0.0038112031761556864}, {'label': 'HS_Individual', 'score': 0.012012024410068989}, {'label': 'HS_Group', 'score': 0.004762946628034115}, {'label': 'HS_Religion', 'score': 0.0032223816961050034}, {'label': 'HS_Race', 'score': 0.004168652463704348}, {'label': 'HS_Physical', 'score': 0.00048205480561591685}, {'label': 'HS_Gender', 'score': 0.000542783469427377}, {'label': 'HS_Other', 'score': 0.008541245013475418}, {'label': 'HS_Weak', 'score': 0.00872738379985094}, {'label': 'HS_Moderate', 'score': 0.0030228530522435904}, {'label': 'HS_Strong', 'score': 0.0023450893349945545}]]\n",
      "Text: 'Selamat pagi semua!'\n",
      "   Expected: Clean\n",
      "   üö® Hate Speech: ‚úÖ NO (0.016) - low\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.008176149800419807}, {'label': 'Abusive', 'score': 0.0027387477457523346}, {'label': 'HS_Individual', 'score': 0.006026771385222673}, {'label': 'HS_Group', 'score': 0.002971058012917638}, {'label': 'HS_Religion', 'score': 0.0020135389640927315}, {'label': 'HS_Race', 'score': 0.002004530979320407}, {'label': 'HS_Physical', 'score': 0.0003330110339447856}, {'label': 'HS_Gender', 'score': 0.0003693913167808205}, {'label': 'HS_Other', 'score': 0.004670690279453993}, {'label': 'HS_Weak', 'score': 0.004574993159621954}, {'label': 'HS_Moderate', 'score': 0.002176556270569563}, {'label': 'HS_Strong', 'score': 0.0011345385573804379}]]\n",
      "Text: 'Terima kasih atas bantuannya'\n",
      "   Expected: Clean\n",
      "   üö® Hate Speech: ‚úÖ NO (0.008) - low\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.026395505294203758}, {'label': 'Abusive', 'score': 0.01087238546460867}, {'label': 'HS_Individual', 'score': 0.02919437736272812}, {'label': 'HS_Group', 'score': 0.001898844144307077}, {'label': 'HS_Religion', 'score': 0.0012790595646947622}, {'label': 'HS_Race', 'score': 0.0007396867731586099}, {'label': 'HS_Physical', 'score': 0.0002988287014886737}, {'label': 'HS_Gender', 'score': 0.0003016763657797128}, {'label': 'HS_Other', 'score': 0.026694944128394127}, {'label': 'HS_Weak', 'score': 0.02115461602807045}, {'label': 'HS_Moderate', 'score': 0.0018761014798656106}, {'label': 'HS_Strong', 'score': 0.0008029469754546881}]]\n",
      "Text: 'Dasar bodoh tidak tahu apa-apa'\n",
      "   Expected: Abusive/HS_Individual\n",
      "   üö® Hate Speech: ‚úÖ NO (0.029) - low\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.035234544426202774}, {'label': 'Abusive', 'score': 0.0023976864758878946}, {'label': 'HS_Individual', 'score': 0.003471940755844116}, {'label': 'HS_Group', 'score': 0.027873802930116653}, {'label': 'HS_Religion', 'score': 0.03345653414726257}, {'label': 'HS_Race', 'score': 0.005262589547783136}, {'label': 'HS_Physical', 'score': 0.00030912039801478386}, {'label': 'HS_Gender', 'score': 0.000431761727668345}, {'label': 'HS_Other', 'score': 0.0027212698478251696}, {'label': 'HS_Weak', 'score': 0.0024995950516313314}, {'label': 'HS_Moderate', 'score': 0.019558433443307877}, {'label': 'HS_Strong', 'score': 0.003014323301613331}]]\n",
      "Text: 'Agama kalian sesat semua'\n",
      "   Expected: HS_Religion/HS_Group\n",
      "   üö® Hate Speech: ‚úÖ NO (0.035) - low\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.015162281692028046}, {'label': 'Abusive', 'score': 0.0033614214044064283}, {'label': 'HS_Individual', 'score': 0.007615985814481974}, {'label': 'HS_Group', 'score': 0.006300174631178379}, {'label': 'HS_Religion', 'score': 0.00281955162063241}, {'label': 'HS_Race', 'score': 0.004170163068920374}, {'label': 'HS_Physical', 'score': 0.0004234279622323811}, {'label': 'HS_Gender', 'score': 0.0005079182446934283}, {'label': 'HS_Other', 'score': 0.01000986061990261}, {'label': 'HS_Weak', 'score': 0.0071112168952822685}, {'label': 'HS_Moderate', 'score': 0.004573638550937176}, {'label': 'HS_Strong', 'score': 0.0018083543982356787}]]\n",
      "Text: 'Perempuan memang inferior'\n",
      "   Expected: HS_Gender\n",
      "   üö® Hate Speech: ‚úÖ NO (0.015) - low\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.8648852705955505}, {'label': 'Abusive', 'score': 0.9882041215896606}, {'label': 'HS_Individual', 'score': 0.7469054460525513}, {'label': 'HS_Group', 'score': 0.033975034952163696}, {'label': 'HS_Religion', 'score': 0.02406526356935501}, {'label': 'HS_Race', 'score': 0.006647973787039518}, {'label': 'HS_Physical', 'score': 0.10878635197877884}, {'label': 'HS_Gender', 'score': 0.06266186386346817}, {'label': 'HS_Other', 'score': 0.16982345283031464}, {'label': 'HS_Weak', 'score': 0.7884725332260132}, {'label': 'HS_Moderate', 'score': 0.04536602273583412}, {'label': 'HS_Strong', 'score': 0.0024157268926501274}]]\n",
      "Text: 'Orang ras itu memang jelek'\n",
      "   Expected: HS_Race\n",
      "   üö® Hate Speech: üö® YES (0.988) - high\n",
      "   üè∑Ô∏è Categories: HS, Abusive, HS_Individual, HS_Weak\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.010290510021150112}, {'label': 'Abusive', 'score': 0.786493718624115}, {'label': 'HS_Individual', 'score': 0.013693682849407196}, {'label': 'HS_Group', 'score': 0.001881378935649991}, {'label': 'HS_Religion', 'score': 0.0006591433193534613}, {'label': 'HS_Race', 'score': 0.0010207424638792872}, {'label': 'HS_Physical', 'score': 0.0029774168506264687}, {'label': 'HS_Gender', 'score': 0.002514052204787731}, {'label': 'HS_Other', 'score': 0.007287383545190096}, {'label': 'HS_Weak', 'score': 0.014722837135195732}, {'label': 'HS_Moderate', 'score': 0.002063001738861203}, {'label': 'HS_Strong', 'score': 0.00042156342533417046}]]\n",
      "Text: 'Bunuh saja dia'\n",
      "   Expected: HS_Strong\n",
      "   üö® Hate Speech: üö® YES (0.786) - medium\n",
      "   üè∑Ô∏è Categories: Abusive\n",
      "\n",
      "Debug - Raw result type: <class 'list'>\n",
      "Debug - Raw result: [[{'label': 'HS', 'score': 0.01610260270535946}, {'label': 'Abusive', 'score': 0.08261874318122864}, {'label': 'HS_Individual', 'score': 0.017462853342294693}, {'label': 'HS_Group', 'score': 0.0016208930173888803}, {'label': 'HS_Religion', 'score': 0.0009445871692150831}, {'label': 'HS_Race', 'score': 0.0006024299073033035}, {'label': 'HS_Physical', 'score': 0.0005559123819693923}, {'label': 'HS_Gender', 'score': 0.0005030823522247374}, {'label': 'HS_Other', 'score': 0.009527668356895447}, {'label': 'HS_Weak', 'score': 0.016417071223258972}, {'label': 'HS_Moderate', 'score': 0.0012569507816806436}, {'label': 'HS_Strong', 'score': 0.0003428662894293666}]]\n",
      "Text: 'Agak aneh sih orangnya'\n",
      "   Expected: HS_Weak\n",
      "   üö® Hate Speech: ‚úÖ NO (0.083) - low\n",
      "\n",
      "‚úÖ HateSpeechAnalyzer test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test HateSpeechAnalyzer only - FIXED VERSION\n",
    "\n",
    "print(\"üöÄ Testing HateSpeechAnalyzer...\")\n",
    "\n",
    "# Initialize\n",
    "try:\n",
    "    # Buat versi yang lebih simple untuk testing\n",
    "    hate_analyzer = HateSpeechAnalyzer()\n",
    "\n",
    "    # Test with hate speech examples\n",
    "    hate_test_texts = [\n",
    "        (\"Selamat pagi semua!\", \"Expected: Clean\"),\n",
    "        (\"Terima kasih atas bantuannya\", \"Expected: Clean\"),\n",
    "        (\"Dasar bodoh tidak tahu apa-apa\", \"Expected: Abusive/HS_Individual\"),\n",
    "        (\"Agama kalian sesat semua\", \"Expected: HS_Religion/HS_Group\"),\n",
    "        (\"Perempuan memang inferior\", \"Expected: HS_Gender\"),\n",
    "        (\"Orang ras itu memang jelek\", \"Expected: HS_Race\"),\n",
    "        (\"Bunuh saja dia\", \"Expected: HS_Strong\"),\n",
    "        (\"Agak aneh sih orangnya\", \"Expected: HS_Weak\"),\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüß™ Testing individual hate speech predictions:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for text, expected in hate_test_texts:\n",
    "        # Hapus debug print untuk testing yang clean\n",
    "        result = hate_analyzer.predict_single(text, threshold=0.5)\n",
    "\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"   {expected}\")\n",
    "        hate_status = \"üö® YES\" if result[\"is_hate_speech\"] else \"‚úÖ NO\"\n",
    "        print(\n",
    "            f\"   üö® Hate Speech: {hate_status} ({result['max_score']:.3f}) - {result['confidence']}\"\n",
    "        )\n",
    "        if result[\"categories\"]:\n",
    "            print(f\"   üè∑Ô∏è Categories: {', '.join(result['categories'])}\")\n",
    "        print()\n",
    "\n",
    "    print(\"‚úÖ HateSpeechAnalyzer test completed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in HateSpeechAnalyzer: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f268b4",
   "metadata": {},
   "source": [
    "# BERT Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0f69aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengunduh model (jika diperlukan)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cahya/bert-base-indonesian-1.5G were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model siap digunakan.\n",
      "\n",
      "--- Tes untuk: 'Ibu kota negara Indonesia adalah [MASK].' ---\n",
      "Kata: jakarta         | Skor Keyakinan: 0.5408\n",
      "Kata: yogyakarta      | Skor Keyakinan: 0.0404\n",
      "Kata: pontianak       | Skor Keyakinan: 0.0294\n",
      "Kata: makassar        | Skor Keyakinan: 0.0170\n",
      "Kata: merauke         | Skor Keyakinan: 0.0158\n",
      "\n",
      "--- Tes untuk: 'Orang yang bekerja di rumah sakit biasanya adalah seorang [MASK].' ---\n",
      "Kalimat Lengkap: orang yang bekerja di rumah sakit biasanya adalah seorang dokter.\n",
      "Kalimat Lengkap: orang yang bekerja di rumah sakit biasanya adalah seorang perawat.\n",
      "Kalimat Lengkap: orang yang bekerja di rumah sakit biasanya adalah seorang bidan.\n",
      "\n",
      "--- Tes untuk: 'Setelah lelah bekerja seharian, enaknya minum [MASK] dingin.' ---\n",
      "Kalimat Lengkap: setelah lelah bekerja seharian, enaknya minum air dingin.\n",
      "Kalimat Lengkap: setelah lelah bekerja seharian, enaknya minum minuman dingin.\n",
      "Kalimat Lengkap: setelah lelah bekerja seharian, enaknya minum teh dingin.\n",
      "\n",
      "--- Tes untuk: 'Dia membeli mobil baru berwarna [MASK].' ---\n",
      "Kalimat Lengkap: dia membeli mobil baru berwarna putih.\n",
      "Kalimat Lengkap: dia membeli mobil baru berwarna hitam.\n",
      "Kalimat Lengkap: dia membeli mobil baru berwarna merah.\n",
      "Kalimat Lengkap: orang yang bekerja di rumah sakit biasanya adalah seorang dokter.\n",
      "Kalimat Lengkap: orang yang bekerja di rumah sakit biasanya adalah seorang perawat.\n",
      "Kalimat Lengkap: orang yang bekerja di rumah sakit biasanya adalah seorang bidan.\n",
      "\n",
      "--- Tes untuk: 'Setelah lelah bekerja seharian, enaknya minum [MASK] dingin.' ---\n",
      "Kalimat Lengkap: setelah lelah bekerja seharian, enaknya minum air dingin.\n",
      "Kalimat Lengkap: setelah lelah bekerja seharian, enaknya minum minuman dingin.\n",
      "Kalimat Lengkap: setelah lelah bekerja seharian, enaknya minum teh dingin.\n",
      "\n",
      "--- Tes untuk: 'Dia membeli mobil baru berwarna [MASK].' ---\n",
      "Kalimat Lengkap: dia membeli mobil baru berwarna putih.\n",
      "Kalimat Lengkap: dia membeli mobil baru berwarna hitam.\n",
      "Kalimat Lengkap: dia membeli mobil baru berwarna merah.\n"
     ]
    }
   ],
   "source": [
    "# 1. Inisialisasi pipeline \"fill-mask\"\n",
    "# Ini akan mengunduh model jika belum ada di cache\n",
    "print(\"Mengunduh model (jika diperlukan)...\")\n",
    "tebak_kata = pipeline(\"fill-mask\", model=\"cahya/bert-base-indonesian-1.5G\")\n",
    "print(\"Model siap digunakan.\")\n",
    "\n",
    "# 2. Siapkan beberapa kalimat tes\n",
    "kalimat1 = \"Ibu kota negara Indonesia adalah [MASK].\"\n",
    "kalimat2 = \"Orang yang bekerja di rumah sakit biasanya adalah seorang [MASK].\"\n",
    "kalimat3 = \"Setelah lelah bekerja seharian, enaknya minum [MASK] dingin.\"\n",
    "kalimat4 = \"Dia membeli mobil baru berwarna [MASK].\"\n",
    "\n",
    "# 3. Lakukan prediksi dan lihat hasilnya\n",
    "print(f\"\\n--- Tes untuk: '{kalimat1}' ---\")\n",
    "hasil1 = tebak_kata(kalimat1)\n",
    "for prediksi in hasil1:\n",
    "    print(\n",
    "        f\"Kata: {prediksi['token_str']:<15} | Skor Keyakinan: {prediksi['score']:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n--- Tes untuk: '{kalimat2}' ---\")\n",
    "hasil2 = tebak_kata(kalimat2, top_k=3)  # Minta 3 tebakan teratas\n",
    "for prediksi in hasil2:\n",
    "    print(f\"Kalimat Lengkap: {prediksi['sequence']}\")\n",
    "\n",
    "print(f\"\\n--- Tes untuk: '{kalimat3}' ---\")\n",
    "hasil3 = tebak_kata(kalimat3, top_k=3)\n",
    "for prediksi in hasil3:\n",
    "    print(f\"Kalimat Lengkap: {prediksi['sequence']}\")\n",
    "\n",
    "print(f\"\\n--- Tes untuk: '{kalimat4}' ---\")\n",
    "hasil4 = tebak_kata(kalimat4, top_k=3)\n",
    "for prediksi in hasil4:\n",
    "    print(f\"Kalimat Lengkap: {prediksi['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8e42d",
   "metadata": {},
   "source": [
    "# üåç XLM-RoBERTa Model Testing\n",
    "\n",
    "Testing Facebook's XLM-RoBERTa (Cross-lingual RoBERTa) model which supports 100 languages including Indonesian. This model can be useful for:\n",
    "- Text classification tasks in multiple languages\n",
    "- Feature extraction for bot detection\n",
    "- Cross-lingual text analysis\n",
    "- Masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bea702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ transformers already installed\n",
      "‚úÖ torch already installed\n",
      "\n",
      "üéâ All packages ready for XLM-RoBERTa testing!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for XLM-RoBERTa\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "\n",
    "# Install transformers if not already installed\n",
    "install_package(\"transformers\")\n",
    "install_package(\"torch\")\n",
    "\n",
    "print(\"\\nüéâ All packages ready for XLM-RoBERTa testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5963c542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading XLM-RoBERTa Model...\n",
      "‚ö†Ô∏è  This might take a few minutes for the first time (downloading ~2.2GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XLM-RoBERTa pipeline loaded successfully!\n",
      "\n",
      "üá∫üá∏ English test: 'I think this account is a <mask>.'\n",
      "  1. I think this account is a scam . (score: 0.6098)\n",
      "  2. I think this account is a fake . (score: 0.2386)\n",
      "  3. I think this account is a fraud . (score: 0.0285)\n",
      "  4. I think this account is a troll . (score: 0.0239)\n",
      "  5. I think this account is a hack . (score: 0.0065)\n",
      "\n",
      "üáÆüá© Indonesian test: 'Saya pikir akun ini adalah <mask>.'\n",
      "  1. Saya pikir akun ini adalah palsu . (score: 0.4199)\n",
      "  2. Saya pikir akun ini adalah penipuan . (score: 0.1155)\n",
      "  3. Saya pikir akun ini adalah scam . (score: 0.0412)\n",
      "  4. Saya pikir akun ini adalah ilegal . (score: 0.0322)\n",
      "  5. Saya pikir akun ini adalah salah . (score: 0.0210)\n",
      "\n",
      "üá®üá≥ Chinese test: 'ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™<mask>„ÄÇ'\n",
      "  1. I think this account is a scam . (score: 0.6098)\n",
      "  2. I think this account is a fake . (score: 0.2386)\n",
      "  3. I think this account is a fraud . (score: 0.0285)\n",
      "  4. I think this account is a troll . (score: 0.0239)\n",
      "  5. I think this account is a hack . (score: 0.0065)\n",
      "\n",
      "üáÆüá© Indonesian test: 'Saya pikir akun ini adalah <mask>.'\n",
      "  1. Saya pikir akun ini adalah palsu . (score: 0.4199)\n",
      "  2. Saya pikir akun ini adalah penipuan . (score: 0.1155)\n",
      "  3. Saya pikir akun ini adalah scam . (score: 0.0412)\n",
      "  4. Saya pikir akun ini adalah ilegal . (score: 0.0322)\n",
      "  5. Saya pikir akun ini adalah salah . (score: 0.0210)\n",
      "\n",
      "üá®üá≥ Chinese test: 'ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™<mask>„ÄÇ'\n",
      "  1. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ÈóÆÈ¢ò „ÄÇ (score: 0.0870)\n",
      "  2. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™Èô∑Èò± „ÄÇ (score: 0.0749)\n",
      "  3. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ÈîôËØØ „ÄÇ (score: 0.0522)\n",
      "  4. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ËøùÊ≥ïË°å‰∏∫ „ÄÇ (score: 0.0352)\n",
      "  5. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ÊºèÊ¥û „ÄÇ (score: 0.0241)\n",
      "\n",
      "üá∏üá¶ Arabic test: 'ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà <mask>.'\n",
      "  1. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿ≠ŸÇŸäŸÇŸä . (score: 0.1821)\n",
      "  2. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿßŸÑÿ≠ŸÇŸäŸÇŸä . (score: 0.0447)\n",
      "  3. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿ¥ÿÆÿµŸä . (score: 0.0445)\n",
      "  4. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿµÿ≠Ÿäÿ≠ . (score: 0.0404)\n",
      "  5. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ŸÖÿ¨ŸáŸàŸÑ . (score: 0.0384)\n",
      "\n",
      "üáØüáµ Japanese test: '„Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ<mask>„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ'\n",
      "  1. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Anonymous „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.1109)\n",
      "  2. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Google „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0340)\n",
      "  3. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ fake „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0332)\n",
      "  4. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Twitter „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0279)\n",
      "  5. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Facebook „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0239)\n",
      "\n",
      "üá∞üá∑ Korean test: 'Ïù¥ Í≥ÑÏ†ïÏùÄ <mask>ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.'\n",
      "  1. Ïù¥ Í≥ÑÏ†ïÏùÄ Anonymous ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0263)\n",
      "  2. Ïù¥ Í≥ÑÏ†ïÏùÄ : ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0217)\n",
      "  3. Ïù¥ Í≥ÑÏ†ïÏùÄ ÏïÑÎãà ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0194)\n",
      "  4. Ïù¥ Í≥ÑÏ†ïÏùÄ ÏïÑÎãôÎãàÎã§ ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0189)\n",
      "  5. Ïù¥ Í≥ÑÏ†ïÏùÄ https ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0136)\n",
      "\n",
      "üá™üá∏ Spanish test: 'Creo que esta cuenta es un <mask>.'\n",
      "  1. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ÈóÆÈ¢ò „ÄÇ (score: 0.0870)\n",
      "  2. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™Èô∑Èò± „ÄÇ (score: 0.0749)\n",
      "  3. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ÈîôËØØ „ÄÇ (score: 0.0522)\n",
      "  4. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ËøùÊ≥ïË°å‰∏∫ „ÄÇ (score: 0.0352)\n",
      "  5. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ÊºèÊ¥û „ÄÇ (score: 0.0241)\n",
      "\n",
      "üá∏üá¶ Arabic test: 'ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà <mask>.'\n",
      "  1. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿ≠ŸÇŸäŸÇŸä . (score: 0.1821)\n",
      "  2. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿßŸÑÿ≠ŸÇŸäŸÇŸä . (score: 0.0447)\n",
      "  3. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿ¥ÿÆÿµŸä . (score: 0.0445)\n",
      "  4. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿµÿ≠Ÿäÿ≠ . (score: 0.0404)\n",
      "  5. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ŸÖÿ¨ŸáŸàŸÑ . (score: 0.0384)\n",
      "\n",
      "üáØüáµ Japanese test: '„Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ<mask>„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ'\n",
      "  1. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Anonymous „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.1109)\n",
      "  2. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Google „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0340)\n",
      "  3. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ fake „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0332)\n",
      "  4. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Twitter „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0279)\n",
      "  5. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Facebook „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0239)\n",
      "\n",
      "üá∞üá∑ Korean test: 'Ïù¥ Í≥ÑÏ†ïÏùÄ <mask>ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.'\n",
      "  1. Ïù¥ Í≥ÑÏ†ïÏùÄ Anonymous ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0263)\n",
      "  2. Ïù¥ Í≥ÑÏ†ïÏùÄ : ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0217)\n",
      "  3. Ïù¥ Í≥ÑÏ†ïÏùÄ ÏïÑÎãà ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0194)\n",
      "  4. Ïù¥ Í≥ÑÏ†ïÏùÄ ÏïÑÎãôÎãàÎã§ ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0189)\n",
      "  5. Ïù¥ Í≥ÑÏ†ïÏùÄ https ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0136)\n",
      "\n",
      "üá™üá∏ Spanish test: 'Creo que esta cuenta es un <mask>.'\n",
      "  1. Creo que esta cuenta es un fake . (score: 0.6725)\n",
      "  2. Creo que esta cuenta es un fraude . (score: 0.0562)\n",
      "  3. Creo que esta cuenta es un spam . (score: 0.0372)\n",
      "  4. Creo que esta cuenta es un troll . (score: 0.0370)\n",
      "  5. Creo que esta cuenta es un falso . (score: 0.0167)\n",
      "  1. Creo que esta cuenta es un fake . (score: 0.6725)\n",
      "  2. Creo que esta cuenta es un fraude . (score: 0.0562)\n",
      "  3. Creo que esta cuenta es un spam . (score: 0.0372)\n",
      "  4. Creo que esta cuenta es un troll . (score: 0.0370)\n",
      "  5. Creo que esta cuenta es un falso . (score: 0.0167)\n"
     ]
    }
   ],
   "source": [
    "# Basic XLM-RoBERTa Model Testing\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "print(\"üöÄ Loading XLM-RoBERTa Model...\")\n",
    "print(\"‚ö†Ô∏è  This might take a few minutes for the first time (downloading ~2.2GB)\")\n",
    "\n",
    "# Method 1: Using pipeline (High-level API)\n",
    "try:\n",
    "    # Load the fill-mask pipeline\n",
    "    xlm_roberta_pipe = pipeline(\n",
    "        \"fill-mask\",\n",
    "        model=\"FacebookAI/xlm-roberta-large\",\n",
    "        tokenizer=\"FacebookAI/xlm-roberta-large\"\n",
    "    )\n",
    "    print(\"‚úÖ XLM-RoBERTa pipeline loaded successfully!\")\n",
    "\n",
    "    # Test with English\n",
    "    english_test = \"I think this account is a <mask>.\"\n",
    "    print(f\"\\nüá∫üá∏ English test: '{english_test}'\")\n",
    "    english_results = xlm_roberta_pipe(english_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(english_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with Indonesian\n",
    "    indonesian_test = \"Saya pikir akun ini adalah <mask>.\"\n",
    "    print(f\"\\nüáÆüá© Indonesian test: '{indonesian_test}'\")\n",
    "    indonesian_results = xlm_roberta_pipe(indonesian_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(indonesian_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with china\n",
    "    chinese_test = \"ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™<mask>„ÄÇ\"\n",
    "    print(f\"\\nüá®üá≥ Chinese test: '{chinese_test}'\")\n",
    "    chinese_results = xlm_roberta_pipe(chinese_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(chinese_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with arabic\n",
    "    arabic_test = \"ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà <mask>.\"\n",
    "    print(f\"\\nüá∏üá¶ Arabic test: '{arabic_test}'\")\n",
    "    arabic_results = xlm_roberta_pipe(arabic_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(arabic_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with japanese\n",
    "    japanese_test = \"„Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ<mask>„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ\"\n",
    "    print(f\"\\nüáØüáµ Japanese test: '{japanese_test}'\")\n",
    "    japanese_results = xlm_roberta_pipe(japanese_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(japanese_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with korean\n",
    "    korean_test = \"Ïù¥ Í≥ÑÏ†ïÏùÄ <mask>ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\"\n",
    "    print(f\"\\nüá∞üá∑ Korean test: '{korean_test}'\")\n",
    "    korean_results = xlm_roberta_pipe(korean_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(korean_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with spanish\n",
    "    spanish_test = \"Creo que esta cuenta es un <mask>.\"\n",
    "    print(f\"\\nüá™üá∏ Spanish test: '{spanish_test}'\")\n",
    "    spanish_results = xlm_roberta_pipe(spanish_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(spanish_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading pipeline: {e}\")\n",
    "    xlm_roberta_pipe = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8760aaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading XLM-RoBERTa with direct model access...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XLM-RoBERTa tokenizer and model loaded successfully!\n",
      "üìä Model config:\n",
      "   - Vocab size: 250,002\n",
      "   - Max length: 512\n",
      "   - Model parameters: ~560,142,482\n",
      "\n",
      "üîç Testing: 'Replace me by any text you'd like.'\n",
      "   üìù Tokenized: ['<s>', '‚ñÅRe', 'place', '‚ñÅme', '‚ñÅby', '‚ñÅany', '‚ñÅtext', '‚ñÅyou', \"'\", 'd', '‚ñÅlike', '.', '</s>']\n",
      "   üß† Hidden states shape: torch.Size([1, 13, 1024])\n",
      "   üìä Feature vector size per token: 1024\n",
      "\n",
      "üîç Testing: 'Ganti saya dengan teks apa pun yang Anda suka.'\n",
      "   üìù Tokenized: ['<s>', '‚ñÅGan', 'ti', '‚ñÅsaya', '‚ñÅdengan', '‚ñÅteks', '‚ñÅapa', '‚ñÅpun', '‚ñÅyang', '‚ñÅAnda', '‚ñÅsuka', '.', '</s>']\n",
      "   üß† Hidden states shape: torch.Size([1, 13, 1024])\n",
      "   üìä Feature vector size per token: 1024\n",
      "\n",
      "üîç Testing: 'Ganti saya dengan teks apa pun yang Anda suka.'\n",
      "   üìù Tokenized: ['<s>', '‚ñÅGan', 'ti', '‚ñÅsaya', '‚ñÅdengan', '‚ñÅteks', '‚ñÅapa', '‚ñÅpun', '‚ñÅyang', '‚ñÅAnda', '‚ñÅsuka', '.', '</s>']\n",
      "   üß† Hidden states shape: torch.Size([1, 13, 1024])\n",
      "   üìä Feature vector size per token: 1024\n",
      "\n",
      "üîç Testing: '„Åì„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÂ•Ω„Åç„Å™„ÇÇ„ÅÆ„Å´ÁΩÆ„ÅçÊèõ„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ'\n",
      "   üìù Tokenized: ['<s>', '‚ñÅ„Åì„ÅÆ', '„ÉÜ„Ç≠„Çπ„Éà', '„Çí', 'Â•Ω„Åç„Å™', '„ÇÇ„ÅÆ', '„Å´', 'ÁΩÆ„Åç', 'Êèõ„Åà', '„Å¶„Åè„Å†„Åï„ÅÑ', '„ÄÇ', '</s>']\n",
      "   üß† Hidden states shape: torch.Size([1, 13, 1024])\n",
      "   üìä Feature vector size per token: 1024\n",
      "\n",
      "üîç Testing: '„Åì„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÂ•Ω„Åç„Å™„ÇÇ„ÅÆ„Å´ÁΩÆ„ÅçÊèõ„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ'\n",
      "   üìù Tokenized: ['<s>', '‚ñÅ„Åì„ÅÆ', '„ÉÜ„Ç≠„Çπ„Éà', '„Çí', 'Â•Ω„Åç„Å™', '„ÇÇ„ÅÆ', '„Å´', 'ÁΩÆ„Åç', 'Êèõ„Åà', '„Å¶„Åè„Å†„Åï„ÅÑ', '„ÄÇ', '</s>']\n",
      "   üß† Hidden states shape: torch.Size([1, 12, 1024])\n",
      "   üìä Feature vector size per token: 1024\n",
      "   üß† Hidden states shape: torch.Size([1, 12, 1024])\n",
      "   üìä Feature vector size per token: 1024\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Direct model loading (Lower-level API)\n",
    "print(\"üîß Loading XLM-RoBERTa with direct model access...\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model directly\n",
    "    xlm_tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large\")\n",
    "    xlm_model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/xlm-roberta-large\")\n",
    "\n",
    "    print(\"‚úÖ XLM-RoBERTa tokenizer and model loaded successfully!\")\n",
    "    print(f\"üìä Model config:\")\n",
    "    print(f\"   - Vocab size: {xlm_tokenizer.vocab_size:,}\")\n",
    "    print(f\"   - Max length: {xlm_tokenizer.model_max_length}\")\n",
    "    print(f\"   - Model parameters: ~{sum(p.numel() for p in xlm_model.parameters()):,}\")\n",
    "\n",
    "    # Test direct usage\n",
    "    def test_xlm_roberta_direct(text):\n",
    "        print(f\"\\nüîç Testing: '{text}'\")\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = xlm_tokenizer(text, return_tensors='pt')\n",
    "        print(f\"   üìù Tokenized: {xlm_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "\n",
    "        # Get embeddings/features\n",
    "        with torch.no_grad():\n",
    "            outputs = xlm_model(**inputs, output_hidden_states=True)\n",
    "            # Get the hidden states (features)\n",
    "            hidden_states = outputs.hidden_states[-1]  # Last layer hidden states\n",
    "\n",
    "        print(f\"   üß† Hidden states shape: {hidden_states.shape}\")\n",
    "        print(f\"   üìä Feature vector size per token: {hidden_states.shape[-1]}\")\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "    # Test with different languages\n",
    "    test_texts = [\n",
    "        \"Replace me by any text you'd like.\",  # English\n",
    "        \"Ganti saya dengan teks apa pun yang Anda suka.\",  # Indonesian\n",
    "        \"„Åì„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÂ•Ω„Åç„Å™„ÇÇ„ÅÆ„Å´ÁΩÆ„ÅçÊèõ„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\"  # Japanese\n",
    "    ]\n",
    "\n",
    "    for text in test_texts:\n",
    "        features = test_xlm_roberta_direct(text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model directly: {e}\")\n",
    "    xlm_tokenizer = None\n",
    "    xlm_model = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
