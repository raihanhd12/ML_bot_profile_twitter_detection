{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b810ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"nahiar/twitter_bot_detection\")\n",
    "\n",
    "# Or convert to pandas DataFrame to use head()\n",
    "df = data[\"train\"].to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ec154",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca9a1a",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.info()\n",
    "print(\"Dataset Columns:\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b7464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tampilkan masing masing column itu tuh kayak yang unique value nya berapa\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"{column}: {len(unique_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99a90e",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FITUR YANG SUDAH ANDA BUAT (INI BAGUS, KITA PERTAHANKAN) ---\n",
    "df[\"is_bot\"] = df[\"account_type\"].map({\"human\": 0, \"bot\": 1})\n",
    "df[\"follower_following_ratio\"] = df[\"followers_count\"] / df[\"friends_count\"].replace(\n",
    "    0, 1\n",
    ")\n",
    "\n",
    "# --- PENINGKATAN & FITUR BARU ---\n",
    "\n",
    "# 1. Dari kolom 'description'\n",
    "# bio_length sudah bagus, mari kita tambahkan lagi. Bot seringkali tidak punya bio atau punya bio yang sangat pendek.\n",
    "df[\"bio_length\"] = df[\"description\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "\n",
    "# 2. Dari kolom 'screen_name'\n",
    "# Panjang username dan jumlah digit di dalamnya adalah sinyal klasik untuk akun bot/spam.\n",
    "df[\"username_length\"] = df[\"screen_name\"].apply(lambda x: len(str(x)))\n",
    "df[\"username_digit_count\"] = df[\"screen_name\"].apply(\n",
    "    lambda x: sum(c.isdigit() for c in str(x))\n",
    ")\n",
    "\n",
    "# 3. Dari kolom boolean/lainnya\n",
    "# has_profile_picture sudah bagus, mari kita buat lebih eksplisit namanya.\n",
    "df[\"has_custom_profile_image\"] = df[\"default_profile_image\"].apply(\n",
    "    lambda x: 0 if x else 1\n",
    ")\n",
    "# Apakah pengguna punya background kustom? Bot biasanya tidak.\n",
    "df[\"has_custom_background\"] = df[\"profile_background_image_url\"].apply(\n",
    "    lambda x: 0 if \"default_profile\" in str(x) or \"theme1/bg.png\" in str(x) else 1\n",
    ")\n",
    "# Apakah ada info lokasi?\n",
    "df[\"has_location\"] = df[\"location\"].apply(\n",
    "    lambda x: 0 if str(x).lower() in [\"unknown\", \"none\", \"\"] else 1\n",
    ")\n",
    "# Apakah Default Profile True? jika iya maka 0\n",
    "df[\"is_default_profile\"] = df[\"default_profile\"].apply(lambda x: 0 if x else 1)\n",
    "\n",
    "# Apakah geo_enabled? Jika iya maka 1, jika tidak maka 0\n",
    "df[\"is_geo_enabled\"] = df[\"geo_enabled\"].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Apakah verified? Jika iya maka 1, jika tidak maka 0\n",
    "df[\"is_verified\"] = df[\"verified\"].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Menampilkan 10 data teratas dengan fitur baru\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec53dd0",
   "metadata": {},
   "source": [
    "## Remove Unused Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd57156",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\n",
    "    columns=[\n",
    "        \"location\",\n",
    "        \"profile_background_image_url\",\n",
    "        \"default_profile_image\",\n",
    "        \"screen_name\",\n",
    "        \"description\",\n",
    "        \"Unnamed: 0\",\n",
    "        \"created_at\",\n",
    "        \"id\",\n",
    "        \"lang\",\n",
    "        \"geo_enabled\",\n",
    "        \"verified\",\n",
    "        \"default_profile\",\n",
    "        \"profile_image_url\"\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "print(\"Columns after removal:\", df.columns)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7005de3",
   "metadata": {},
   "source": [
    "# Data Visualization & Analysis\n",
    "\n",
    "Let's create comprehensive visualizations to understand our bot detection dataset better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11142c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups for analysis and modeling\n",
    "numeric_features = [\n",
    "    'favourites_count', 'followers_count', 'friends_count', 'statuses_count',\n",
    "    'average_tweets_per_day', 'account_age_days', 'follower_following_ratio',\n",
    "    'bio_length', 'username_length', 'username_digit_count'\n",
    "]\n",
    "\n",
    "binary_features = [\n",
    "    'has_custom_profile_image', 'has_custom_background', 'has_location',\n",
    "    'is_default_profile', 'is_geo_enabled', 'is_verified'\n",
    "]\n",
    "\n",
    "all_features = numeric_features + binary_features\n",
    "\n",
    "print(\"Numeric Features:\", len(numeric_features))\n",
    "print(numeric_features)\n",
    "print(\"\\nBinary Features:\", len(binary_features))\n",
    "print(binary_features)\n",
    "print(f\"\\nTotal Features for Model: {len(all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b77409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Bot vs Human Distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot\n",
    "bot_counts = df['is_bot'].value_counts()\n",
    "labels = ['Human', 'Bot']\n",
    "colors = ['#2E86AB', '#A23B72']\n",
    "\n",
    "axes[0].bar(labels, bot_counts.values, color=colors, alpha=0.8)\n",
    "axes[0].set_title('Distribution of Bots vs Humans', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate(bot_counts.values):\n",
    "    axes[0].text(i, v + 100, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(bot_counts.values, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "           startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Percentage Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Bots: {bot_counts[1]:,} ({bot_counts[1]/len(df)*100:.1f}%)\")\n",
    "print(f\"Humans: {bot_counts[0]:,} ({bot_counts[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Bot to Human ratio: 1:{bot_counts[0]/bot_counts[1]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ad98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Distribution Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# Key numeric features comparison\n",
    "key_features = ['followers_count', 'friends_count', 'statuses_count', 'account_age_days']\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    row, col = i // 2, i % 2\n",
    "\n",
    "    # Box plot for each feature by bot/human\n",
    "    df_plot = df[[feature, 'account_type']].copy()\n",
    "\n",
    "    # Handle outliers by capping at 95th percentile for better visualization\n",
    "    cap_value = df_plot[feature].quantile(0.95)\n",
    "    df_plot[feature] = df_plot[feature].clip(upper=cap_value)\n",
    "\n",
    "    sns.boxplot(data=df_plot, x='account_type', y=feature, ax=axes[row, col])\n",
    "    axes[row, col].set_title(f'{feature.replace(\"_\", \" \").title()} Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[row, col].set_xlabel('Account Type')\n",
    "    axes[row, col].set_ylabel(feature.replace(\"_\", \" \").title())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical comparison\n",
    "print(\"=== STATISTICAL COMPARISON (HUMANS vs BOTS) ===\")\n",
    "for feature in key_features:\n",
    "    human_data = df[df['account_type'] == 'human'][feature]\n",
    "    bot_data = df[df['account_type'] == 'bot'][feature]\n",
    "\n",
    "    print(f\"\\n{feature.upper()}:\")\n",
    "    print(f\"  Humans - Mean: {human_data.mean():.2f}, Median: {human_data.median():.2f}\")\n",
    "    print(f\"  Bots   - Mean: {bot_data.mean():.2f}, Median: {bot_data.median():.2f}\")\n",
    "    print(f\"  Difference: {abs(human_data.mean() - bot_data.mean()):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Binary Features Analysis\n",
    "import pandas as pd\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(binary_features):\n",
    "    # Create crosstab for binary feature vs account type\n",
    "    crosstab = pd.crosstab(df[feature], df['account_type'], normalize='columns') * 100\n",
    "\n",
    "    crosstab.plot(kind='bar', ax=axes[i], color=['#A23B72', '#2E86AB'])\n",
    "    axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Percentage (%)')\n",
    "    axes[i].legend(['Bot', 'Human'])\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Binary features statistics\n",
    "print(\"=== BINARY FEATURES ANALYSIS ===\")\n",
    "for feature in binary_features:\n",
    "    human_pct = df[df['account_type'] == 'human'][feature].mean() * 100\n",
    "    bot_pct = df[df['account_type'] == 'bot'][feature].mean() * 100\n",
    "    print(f\"{feature.replace('_', ' ').title()}:\")\n",
    "    print(f\"  Humans: {human_pct:.1f}% | Bots: {bot_pct:.1f}% | Difference: {abs(human_pct - bot_pct):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Correlation Heatmap\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Select all features for correlation\n",
    "correlation_features = all_features + ['is_bot']\n",
    "correlation_matrix = df[correlation_features].corr()\n",
    "\n",
    "# Create heatmap\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix,\n",
    "            mask=mask,\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            cmap='RdBu_r',\n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show top correlations with is_bot\n",
    "bot_correlations = correlation_matrix['is_bot'].drop('is_bot').abs().sort_values(ascending=False)\n",
    "print(\"=== TOP CORRELATIONS WITH BOT DETECTION ===\")\n",
    "for i, (feature, corr) in enumerate(bot_correlations.head(8).items(), 1):\n",
    "    direction = \"positive\" if correlation_matrix['is_bot'][feature] > 0 else \"negative\"\n",
    "    print(f\"{i}. {feature.replace('_', ' ').title()}: {corr:.3f} ({direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56e85b",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation\n",
    "\n",
    "Let's train multiple models and select the best one for bot detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9cf262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = numeric_features + binary_features\n",
    "X = df[feature_columns]\n",
    "y = df['is_bot']\n",
    "\n",
    "print(\"=== DATA PREPARATION ===\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Features used: {feature_columns}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Train target distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"Test target distribution:\\n{y_test.value_counts()}\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = RobustScaler()  # RobustScaler is less sensitive to outliers\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nData preparation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d88ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models including XGBoost and other advanced models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss', n_jobs=-1),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, verbose=-1, n_jobs=-1),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Cross-validation results\n",
    "cv_results = {}\n",
    "cv_scores = {}\n",
    "\n",
    "print(\"=== CROSS-VALIDATION RESULTS ===\")\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    # Use scaled data for SVM, Logistic Regression, and Naive Bayes\n",
    "    # Tree-based models (RF, XGB, LightGBM, Extra Trees, GB, AdaBoost) use original data\n",
    "    if name in ['SVM', 'Logistic Regression', 'Naive Bayes']:\n",
    "        X_cv = X_train_scaled\n",
    "    else:\n",
    "        X_cv = X_train\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_score = cross_val_score(model, X_cv, y_train, cv=kfold, scoring='f1')\n",
    "    cv_results[name] = cv_score\n",
    "    cv_scores[name] = cv_score.mean()\n",
    "\n",
    "    print(f\"{name} - F1 Score: {cv_score.mean():.4f} (+/- {cv_score.std() * 2:.4f})\")\n",
    "\n",
    "# Sort models by performance\n",
    "sorted_models = sorted(cv_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"\\n=== MODEL RANKING (by F1 Score) ===\")\n",
    "for i, (name, score) in enumerate(sorted_models, 1):\n",
    "    print(f\"{i}. {name}: {score:.4f}\")\n",
    "\n",
    "best_model_name = sorted_models[0][0]\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "# Visualize CV results with improved styling\n",
    "plt.figure(figsize=(15, 8))\n",
    "model_names = list(cv_results.keys())\n",
    "cv_scores_list = [cv_results[name] for name in model_names]\n",
    "\n",
    "# Create boxplot with colors\n",
    "box_plot = plt.boxplot(cv_scores_list, labels=model_names, patch_artist=True)\n",
    "colors = plt.cm.Set3(range(len(model_names)))\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "plt.title('Cross-Validation F1 Scores Comparison', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.xlabel('Models', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display top 3 models performance comparison\n",
    "print(f\"\\nüèÜ TOP 3 MODELS:\")\n",
    "for i, (name, score) in enumerate(sorted_models[:3], 1):\n",
    "    print(f\"{i}. {name}: {score:.4f} F1-Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model on full training set\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Use appropriate data for the best model\n",
    "if best_model_name in ['SVM', 'Logistic Regression', 'Naive Bayes']:\n",
    "    X_train_final = X_train_scaled\n",
    "    X_test_final = X_test_scaled\n",
    "else:\n",
    "    X_train_final = X_train\n",
    "    X_test_final = X_test\n",
    "\n",
    "print(f\"=== TRAINING FINAL MODEL: {best_model_name} ===\")\n",
    "best_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test_final)\n",
    "y_pred_proba = best_model.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"\\n=== FINAL MODEL PERFORMANCE ===\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "print(f\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Human', 'Bot']))\n",
    "\n",
    "# Confusion Matrix and Visualizations\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Human', 'Bot'], yticklabels=['Human', 'Bot'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1, 3, 2)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Feature Importance (if available)\n",
    "plt.subplot(1, 3, 3)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=True)\n",
    "\n",
    "    plt.barh(range(len(feature_importance)), feature_importance['importance'])\n",
    "    plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Importance')\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': abs(best_model.coef_[0])\n",
    "    }).sort_values('importance', ascending=True)\n",
    "\n",
    "    plt.barh(range(len(feature_importance)), feature_importance['importance'])\n",
    "    plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
    "    plt.title('Feature Importance (|Coefficients|)')\n",
    "    plt.xlabel('|Coefficient|')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Feature importance\\nnot available for\\nthis model type',\n",
    "             ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    plt.title('Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48408c53",
   "metadata": {},
   "source": [
    "# üöÄ Model Deployment & Hugging Face Upload\n",
    "\n",
    "Now let's save our best model and prepare it for deployment on Hugging Face Hub!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and preprocessing components\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create model directory\n",
    "model_dir = \"twitter_bot_detection_model\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = os.path.join(model_dir, \"best_model.pkl\")\n",
    "joblib.dump(best_model, model_path)\n",
    "\n",
    "# Save the scaler (for consistency in preprocessing)\n",
    "scaler_path = os.path.join(model_dir, \"scaler.pkl\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "# Save feature names and metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'feature_columns': feature_columns,\n",
    "    'numeric_features': numeric_features,\n",
    "    'binary_features': binary_features,\n",
    "    'performance_metrics': {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc_roc': auc\n",
    "    },\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'dataset_size': len(df),\n",
    "    'features_count': len(feature_columns)\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(model_dir, \"model_metadata.json\")\n",
    "import json\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "print(f\"üìÅ Model directory: {model_dir}\")\n",
    "print(f\"ü§ñ Best model: {best_model_name}\")\n",
    "print(f\"üìä Performance: {f1:.4f} F1-Score\")\n",
    "print(f\"üíæ Files saved:\")\n",
    "print(f\"  - {model_path}\")\n",
    "print(f\"  - {scaler_path}\")\n",
    "print(f\"  - {metadata_path}\")\n",
    "\n",
    "# Create a simple prediction function\n",
    "def predict_bot(input_data):\n",
    "    \"\"\"\n",
    "    Predict if a Twitter account is a bot or human\n",
    "\n",
    "    Args:\n",
    "        input_data: dict with keys matching feature_columns\n",
    "\n",
    "    Returns:\n",
    "        dict with prediction and probability\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame\n",
    "    import pandas as pd\n",
    "    df_input = pd.DataFrame([input_data])\n",
    "\n",
    "    # Ensure all features are present\n",
    "    for col in feature_columns:\n",
    "        if col not in df_input.columns:\n",
    "            df_input[col] = 0\n",
    "\n",
    "    # Reorder columns to match training\n",
    "    df_input = df_input[feature_columns]\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = best_model.predict(df_input)[0]\n",
    "    probability = best_model.predict_proba(df_input)[0]\n",
    "\n",
    "    return {\n",
    "        'prediction': 'Bot' if prediction == 1 else 'Human',\n",
    "        'confidence': max(probability),\n",
    "        'bot_probability': probability[1],\n",
    "        'human_probability': probability[0]\n",
    "    }\n",
    "\n",
    "# Test the prediction function\n",
    "sample_data = {\n",
    "    'favourites_count': 1000,\n",
    "    'followers_count': 500,\n",
    "    'friends_count': 200,\n",
    "    'statuses_count': 1500,\n",
    "    'average_tweets_per_day': 2.5,\n",
    "    'account_age_days': 365,\n",
    "    'follower_following_ratio': 2.5,\n",
    "    'bio_length': 120,\n",
    "    'username_length': 12,\n",
    "    'username_digit_count': 2,\n",
    "    'has_custom_profile_image': 1,\n",
    "    'has_custom_background': 1,\n",
    "    'has_location': 1,\n",
    "    'is_default_profile': 1,\n",
    "    'is_geo_enabled': 1,\n",
    "    'is_verified': 0\n",
    "}\n",
    "\n",
    "test_prediction = predict_bot(sample_data)\n",
    "print(f\"\\nüß™ Test Prediction:\")\n",
    "print(f\"Sample account is predicted as: {test_prediction['prediction']}\")\n",
    "print(f\"Confidence: {test_prediction['confidence']:.3f}\")\n",
    "print(f\"Bot probability: {test_prediction['bot_probability']:.3f}\")\n",
    "print(f\"Human probability: {test_prediction['human_probability']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
